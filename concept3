03.쉽게 배우는 경사 하강 학습법

1. 모델의 학습과 최적화 이론
학습은 지도학습과 비지도 학습으로 나뉜다. 
지도학습 :  입력과 함께 정답을 알려주고 그 정답을 맞추도록 하는 학습 방법
비지도학습 :  정답의 제공 없이 학습 데이터로부터 유용한 정보를 추출하는 학습 방법
머신러닝의 지도학습 : 학습데이터와 정답을 같이 제공함. 문제와 정답을 같이 줌

학습 매개변수 : 파라미터, 학습과정에서 값이 변화하는 매개변수, 이 값이 변하면 알고리즘 출력이 변화. 수학적인 식이 학습모델임

손실함수 : 알고리즘이 얼마나 잘못하고 있는지를 표현. 값이 낮을수록 학습이 잘된것 => 리그레션에 사용하는 평균제곱에러(MSE), 분류에 사용하는 교차엔트로피오차

학습환경정의 : 학습에 필요한 세가지 요소는 data,model,loss이다.
데이터셋에는 데이터를 모델로 입력해 줄 수 있는 입력과 로스 평션의 비교값으로 넣어줄 수 있는 정답 레이블이 있어야합니다. 

2.경사하강학습법
최적화이론 : 함수의 출력이 최소 또는 최대가 되게하는 입력값을 찾는 것. 

무차별대입법 : 가능한 모든 수를 대입해보는것, 가장 단순, 범위를 알아야하고 촘촘하게 조사해야하고, 계산 복잡도가 높은 이유로 최적화에 이용할 수 없다. 

경사하강법(gradient  desent) :  경사를 따라 여러 번의 스텝을 통해 최적점으로 다가간다. 경사는 기울기를 이용해 계산

학습률 : 알파에 비례하여 이동, 왼쪽은 너무 느리고 오른쪽은 잘 접근하지 못하고 진동. 잘찾는게 중요

볼록 함수 : 어디서 시작하더라도 경사하강법으로 최적에 도달 가능
비볼록 함수 : 시작 위치에 따라 다름 최적값 찾는다. 각 극값인 지엽 최적값에 빠질 위험이 있다.

3. 최적화 이론과 수학적 표현
최적화 이론의 수학적 이해 : 어떤 함수가 주어짐(손실함수), 두가지 제약조건이 성립해야함. 제약조건이 성립하는 모든 해 중 f(x)를 최소화 해주는 해를 찾는 문제, 연속변수와 불연속 변수에 따라 크게 둘로 나눠짐. 최소화 문제와 최대화 문제, 미분이 가능해야함

분석적 VS 수치척 방법
분석적 : 함수의 모든 구간을 수식으로 알 때 a,b,c,d,e값도 다 알 때, 미분값이 0이고 아래로 볼록인 곳 찾기, 두 점 중에서 최솟값이 발생 대입해서 찾기
수치적 : 분석적으로 모든 구간의 함수값을 알 수 있는 상황이 아니고, 수치적으로 계속해서 알 수 없는 상황에서 함수의 형태와 수식을 알지 못할 때 사용하는 계산적인 방법
=>계산적인 방법의 대표는 그라디언트 디센트 사용

전역 솔루션은 정의역에서 단 하나 존재.
지역솔루션은 여러개 있을 수 있다. 
=>하나의 솔루션을 찾았을 때 로컬인지 글로벌인지 확신할 수 없다. 

딥러닝과 최적화 이론 : 학습입력을 네트워크 구조에 돌리고 나온 값과 정답 레이블을 손실함수를 통해 비교, 알고리즘 옵티마이저를 이용해서 손실함수가 최소가 되도록 하는 파라미터를 구하는 최적화 문제로 볼 수 있다. 

4. 경사하강학습법
수학적으로 레츠고 
미분과 기울기 (그라디언트) : 오른쪽 사진은 x0축, x1축, x2축, xn축까지 각각 편미분해서 벡터로 쪽 늘어놓은 것이 이 그라디언트입니다.   
y축 미분은값은 변하지 않음, x축 방향으로는 값이 증가함.
x축 미분은값은 변하지 않음, y축 방향으로는 값이 증가함.
y, x축 방향으로는 값이 증가함. 대각선 위로 증가하는 미분값 나옴

경사하강법 : 원디의 경우에는 x n-1에  - (학습률 곱하기 n-1번째 미분값)을 한다. 
그러면 최솟값을 따라가도록 한다. 

앤디의 경우도 스칼라를 각각 n개의 벡터로 미분하여 이러한 과정을 한다. 
=> f(x)값이 변하지 않을 때까지 스텝을 반복한다. 

학습률의 선택 : 아까 말했던 것처럼 작은경우 수렴 늦고, 큰경우 진동 일어난다. 
알파가 쫌 더 커지면 발산하는 경우도 나타난다. => x가 nan값이 뜨거나, f(x)가 inf가 나타난다. 

5. 심화 경사 하강 학습법
대부분의 문제는 비볼록 함수이므로 단순 경사하강법으로는 한계가 있다. 
초기값에 따라 로컬 미니멈에 빠질 위험이 있다. 
안장점은 기울기가 0이 되지만 극값이 아닌 지점을 말하는데 기울기가 0이 되므로 경사하강법은 안장점에서 벗어나지 못한다. 

해결하기 위해 여러 테크닉 알아보자
관성(momentum) : 이동 벡터를 이용해 이전 기울기에 영향을 받도록 하는 방법
이동벡터는 관성계수 곱하기 원래속도에 더하기 그라디언트를 한 값입니다. 
관성을 이용하면 로컬 미니멈과 잡음에 대처 가능, 메모리를 두 배로 사용함

적응적 기울기(adagrad) :  변수별로 최적화가 잘 되고, 안되고의 차이가 있기 때문에 이런 것들이 잘 안맞으면 로컬 미니멈에 빠지게 된다. 이러한 문제를 해결하기 위해 변수별로 학습율이 달라지게 조절하는 알고리즘

밑 수식 먼저 보면 그라디언트 디센트를 그하는 식에서 루트 gt+입실론이 포함되었다. 
여기서 g는 이전값에 그라디언트의 제곱을 곱한다. 크기를 누적해서 곱해주는 것이다. 각 변수들에 따라서 얼마나 학습을 했는지 각각 나눠줌으로써 기울기가 커서 학습이 많이 된 변수의 학습율을 감소시켜 다른 변수들이 잘 학습되도록 한다. 
g값이 계속해서 커져서 학습이 오래 진행되면 더이상 학습이 이루어지지 않는 단점도 있다. 

RMSProp (알엠에스프랍)
adagrad(에이다그라드)의 단점을 해결해준다. 합 대신 지수평균을 사용
g값이 다른데 이전값에 학습률을 곱해서 어느정도 값을 줄여주고 나머지 값을 이전값의 제곱에 곱하여 더한다. 이렇게하면 변수 간의 상대적인 학습율 차이는 유지하면서 g값이 무한정 커지지 않아 학습을 오래 할 수 있다. 

Adam(애덤) : RMSProp Momentum의 장점을 결합한 알고리즘
가장 최신의 기술이며 딥러닝에서 옵티마이저를 애덤을 많이 사용한다. 

x먼저 보면 이전 값에서 알엠에스 프랍에 있던 이것과 mt는 관성처럼 작용하여 이 두 가지를 곱한 것을 빼준다. 
여기서 햇은 초기값이 0인것을 보정한 값입니다. 











