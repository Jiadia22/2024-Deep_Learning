{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsn3K+0JnYREQtGpxTOPS2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiadia22/seminor/blob/main/seminor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[전체 구조 및 학습 과정]\n",
        "데이터->(모델->예측->평가->최적화->모델에 적용을 반복)->결과\n",
        "\n",
        "1.data\n",
        "전처리, 변형을 함\n",
        "batch로 만들어서 model에 넣어줌\n",
        "\n",
        "2. model\n",
        "LeNet, AlexNet, VGG, ResNet 등 다양하게 설계된 모델\n",
        "Convolution Layer, Pooling 등 다양한 Layer 층들로 구성\n",
        "이 모델 안에 학습 파라미터가 있고, 이 모델이 학습하는 대상\n",
        "\n",
        "3. prediction/logit\n",
        "각 class별로 예측한 값, 여기서 가장 높은 값이 모델이 예상하는 class, 즉 정답\n",
        "\n",
        "4. Loss/Cost\n",
        "예측값과 정답과 비교해서 얼마나 틀렸는지 확인\n",
        "Cross Enrtropy 등 다양한 Loss Function이 있음\n",
        "계산 통해 나오는 값이 Loss(Cost, Cost Value 등)이라고 불림\n",
        "이 Loss는 얼마나 틀렸는지를 말하며 이 값을 최대한 줄이는 것이 학습의 과정\n",
        "\n",
        "5.Optimization\n",
        "Loss를 최소화하기 위해 기울기를 받아 최적화된 Variable값들로 반환\n",
        "이 반환값이 적용된 모델은 바로 전에 돌렸을 때의 결과보다 나아짐\n",
        "바로 최적화된 값만큼 움직이는게 아니라 Learning Rate 만큼 움직인 값이 적용\n",
        "\n",
        "6. result\n",
        "예측값에서 argmax를 통해 가장 높은 값을 예측한 class라고 둠\n",
        "\n",
        "[딥러닝 용어]\n",
        "-modle(CNN)\n",
        "convolution neural network\n",
        "\n",
        "-Layer\n",
        "층을 여러개 쌓아서 딥러닝이 됨\n",
        "인풋레이어-히든레이어(컨볼루션, 풀링, 액티비셜 펑션 등)-아웃풋레이어\n",
        "히든레이어를 어떻게 쌓나에 대해 모델이 정해짐\n",
        "\n",
        "-convolution(합성곱)\n",
        "weight-학습하려고 하는 대상\n",
        "filter\n",
        "kernel\n",
        "variable\n",
        "bias\n",
        "\n",
        "-Pooling Layer\n",
        "feature를 사이즈를 줄여줌(압축)\n",
        "\n",
        "-Optimization\n",
        "adam,sgd\n",
        "loss를 줄일 수 있도록 도와줌\n",
        "\n",
        "-Activation Function\n",
        "sigmoid\n",
        "relu (음수값 다 제거하면서 불필요한거 줄여줌)\n",
        "\n",
        "-softmax\n",
        "모든값들을 다 합치면 1로 만들어줌( 확률로 만들어줌)\n",
        "\n",
        "-Cost/Loss/Loss Function\n",
        "얼마나 틀렸는지\n",
        "\n",
        "-Learning Rate\n",
        "잘 조절해야함\n",
        "\n",
        "-Batch Size\n",
        "32, 64, 128개\n",
        "\n",
        "-Epoch/Step\n",
        "에폭 수만큼 반복해서 봄, 전체이미지를\n",
        "\n",
        "-train/validation/test\n",
        "data set->train set : test set = 7 : 3 이런느낌으로 나눔\n",
        "train set - eval - test set\n",
        "\n",
        "-Label/Ground Truth\n",
        "레이블에 대한 정답이 있어야 함\n",
        "\n",
        "[CNN 모델 구조]\n",
        "convolution-relu-max pulling\n",
        "\n",
        "feature extraction / classification\n",
        "특징 추출 / 결정 내림\n",
        "\n",
        "convolution layer\n",
        "필터\n",
        "\n",
        "pooling layer (max pooling)\n",
        "하나씩 숫자 높은걸 뽑아서 다시 새롭게 만듬\n",
        "\n",
        "activation function (relu)\n",
        "0보다 작은거 없앰\n",
        "\n",
        "fully connected\n",
        "예측함\n",
        "\n",
        "LeNet\n",
        "AlexNet\n",
        "VGG16\n",
        "ResNet\n",
        "다 컨볼루션과 풀링을 어떻게 쌓나 차이\n",
        "\n"
      ],
      "metadata": {
        "id": "2N3B60pXmsaQ"
      }
    }
  ]
}