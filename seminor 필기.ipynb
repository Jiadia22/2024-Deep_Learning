{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMleog0/6UfOQhAIAq1x8+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiadia22/seminor/blob/main/seminor%20%ED%95%84%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[전체 구조 및 학습 과정]\n",
        "\n",
        "데이터->(모델->예측->평가->최적화->모델에 적용을 반복)->결과\n",
        "\n",
        "1.data\n",
        "전처리, 변형을 함\n",
        "batch로 만들어서 model에 넣어줌\n",
        "\n",
        "2.model\n",
        "LeNet, AlexNet, VGG, ResNet 등 다양하게 설계된 모델\n",
        "Convolution Layer, Pooling 등 다양한 Layer 층들로 구성\n",
        "이 모델 안에 학습 파라미터가 있고, 이 모델이 학습하는 대상\n",
        "\n",
        "3.prediction/logit\n",
        "각 class별로 예측한 값, 여기서 가장 높은 값이 모델이 예상하는 class, 즉 정답\n",
        "\n",
        "4.Loss/Cost\n",
        "예측값과 정답과 비교해서 얼마나 틀렸는지 확인\n",
        "Cross Enrtropy 등 다양한 Loss Function이 있음\n",
        "계산 통해 나오는 값이 Loss(Cost, Cost Value 등)이라고 불림\n",
        "이 Loss는 얼마나 틀렸는지를 말하며 이 값을 최대한 줄이는 것이 학습의 과정\n",
        "\n",
        "5.Optimization(adam같은)\n",
        "Loss를 최소화하기 위해 기울기를 받아 최적화된 Variable값들로 반환\n",
        "이 반환값이 적용된 모델은 바로 전에 돌렸을 때의 결과보다 나아짐\n",
        "바로 최적화된 값만큼 움직이는게 아니라 Learning Rate 만큼 움직인 값이 적용\n",
        "\n",
        "6.result\n",
        "예측값에서 argmax를 통해 가장 높은 값을 예측한 class라고 둠\n",
        "\n",
        "\n",
        "[딥러닝 용어]\n",
        "\n",
        "-modle(CNN)\n",
        "convolution neural network\n",
        "\n",
        "-Layer\n",
        "층을 여러개 쌓아서 딥러닝이 됨\n",
        "인풋레이어-히든레이어(컨볼루션, 풀링, 액티비셜 펑션 등)-아웃풋레이어\n",
        "히든레이어를 어떻게 쌓나에 대해 모델이 정해짐\n",
        "\n",
        "[[컨볼루션 레이어(Convolutional Layer):\n",
        "\n",
        "역할: 이미지에서 지역적인 특징을 감지하고 추출하는 데 사용됩니다.\n",
        "\n",
        "풀링 레이어(Pooling Layer):\n",
        "역할: 특징 맵의 크기를 줄이거나 중요한 정보를 강조하는 데 사용됩니다.\n",
        "\n",
        "활성화 함수(Activation Function):\n",
        "역할: 신경망의 비선형성을 도입하여 모델이 더 복잡한 패턴을 학습할 수 있도록 합니다.\n",
        "\n",
        "완전 연결 레이어(Fully Connected Layer):\n",
        "역할: 특징을 기반으로 최종 예측을 수행합니다.]]\n",
        "\n",
        "-convolution(합성곱)\n",
        "\n",
        "weight=파라미터=커널 -학습하려고 하는 대상\n",
        "filter\n",
        "kernel\n",
        "variable\n",
        "bias\n",
        "\n",
        "Weight (가중치):\n",
        "Weight는 각 입력 값과 연결된 매개변수로, 합성곱 연산에서 필터(커널)의 가중치를 의미합니다.\n",
        "Weight는 모델이 학습하는 대상으로, 학습 과정에서 최적화되어 입력과 출력 간의 관계를 학습하게 됩니다.\n",
        "\n",
        "파라미터(Parameter):\n",
        "Weight, bias 등의 모델의 학습 가능한 변수들을 통틀어 파라미터라고 부릅니다.\n",
        "파라미터는 모델이 입력에서 출력을 생성하는 데 사용되며, 학습 과정에서 조정됩니다.\n",
        "\n",
        "커널 (Kernel):\n",
        "Convolution 연산에서 사용되는 필터로, 가중치의 집합입니다.\n",
        "커널은 주로 작은 크기의 행렬로 정의되며, 입력 데이터에 적용되어 특징을 추출하는 데 사용됩니다.\n",
        "\n",
        "필터 (Filter):\n",
        "Convolution 연산에서 사용되는 가중치(Weight)와 커널을 통칭하여 필터라고 부릅니다.\n",
        "필터는 입력 데이터를 스캔하면서 특징을 감지하고 추출하는 역할을 합니다.\n",
        "\n",
        "Variable (변수):\n",
        "Weight, bias, 커널 등 모델이 학습하는 대상을 변수라고 부릅니다.\n",
        "변수는 모델의 가중치와 편향을 나타내며, 학습 중에 최적화되어 문제에 맞는 최적의 값으로 조정됩니다.\n",
        "\n",
        "Bias (편향):\n",
        "편향은 각 뉴런에 더해지는 상수값으로, 모델이 복잡한 패턴을 학습할 수 있도록 도와줍니다.\n",
        "각 필터마다 하나의 편향이 존재하며, 이는 해당 필터가 특정 특징에 얼마나 민감한지를 조절합니다.\n",
        "\n",
        "\n",
        "\n",
        "-Pooling Layer\n",
        "feature를 사이즈를 줄여줌(압축)\n",
        "\n",
        "-Optimization\n",
        "adam,sgd\n",
        "loss를 줄일 수 있도록 도와줌\n",
        "\n",
        "-Activation Function\n",
        "sigmoid\n",
        "relu (음수값 다 제거하면서 불필요한거 줄여줌)\n",
        "\n",
        "-softmax\n",
        "모든값들을 다 합치면 1로 만들어줌( 확률로 만들어줌)\n",
        "\n",
        "-Cost/Loss/Loss Function\n",
        "얼마나 틀렸는지\n",
        "\n",
        "-Learning Rate\n",
        "잘 조절해야함\n",
        "=>Descent = -Learning Rate * Gradient\n",
        "\n",
        "-Batch Size\n",
        "32, 64, 128개\n",
        "\n",
        "-Epoch/Step\n",
        "에폭 수만큼 반복해서 봄, 전체이미지를\n",
        "\n",
        "-train/validation/test\n",
        "data set->train set : test set = 7 : 3 이런느낌으로 나눔\n",
        "train set - eval - test set\n",
        "\n",
        "-Label/Ground Truth\n",
        "레이블에 대한 정답이 있어야 함\n",
        "\n",
        "[CNN 모델 구조]\n",
        "\n",
        "convolution-relu-max pulling\n",
        "\n",
        "feature extraction / classification\n",
        "특징 추출 / 결정 내림\n",
        "\n",
        "convolution layer\n",
        "필터\n",
        "\n",
        "pooling layer (max pooling)\n",
        "하나씩 숫자 높은걸 뽑아서 다시 새롭게 만듬\n",
        "\n",
        "activation function (relu)\n",
        "0보다 작은거 없앰\n",
        "\n",
        "fully connected\n",
        "예측함\n",
        "\n",
        "LeNet\n",
        "AlexNet\n",
        "VGG16\n",
        "ResNet\n",
        "다 컨볼루션과 풀링을 어떻게 쌓나 차이"
      ],
      "metadata": {
        "id": "BL2xb1f46Icb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.얕은 신경망의 구조\n",
        "\n",
        "인공신경망 - 뉴런들이 모여 서로 연결된 형태 (노드들이 연결된 상태)\n",
        "\n",
        "fully-connected layer - 뉴런이 모인 한 단위를 계층(layer)라 하고, 이전 계층과 모든 뉴런이 서로 연결된 계층을 전결합 계층\n",
        "\n",
        "얕은 신경망 - 입력,은닉,출력 계층으로 되어있음, 은닉 계층과 출력 계층이 fully connected 계층인 모델을 말한다.\n",
        "\n",
        "2.얕은 신경망을 이용한 분류와 회기\n",
        "회기 - 잡음이 있는 샘플로부터(x와y 사이의 규칙이 명확하지 않다) 규칙 찾아 연속된 값의 출력을 추정하는 것.\n",
        "\n",
        "분류 - 입력 값을 분석해 특정 범주(category)로 구분하는 작업 (이진 분류, 다중 분류)\n",
        "\n",
        "[얕은 신경망의 동작은 출력 계층의 활성 함수에 의해 달라진다.]\n",
        "\n",
        "얕은 신경망을 이용한 회귀 - 전 범위의 연속된 값을 출력하므로, 보통 identity함수 사용\n",
        "\n",
        "얕은 신경망을 이용한 이진 분류 - 이진 분류를 위한 출력은 0~1 사이의 실수 값, 활성함수 sigmoid function (0.5보다 작다/크다)\n",
        "\n",
        "얕은 신경망을 이용한 다중 클래스 분류 - softmax 활성 함수를 이용해 해결 가능\n",
        "\n",
        "3.얕은 신경망의 수식적 이해\n",
        "\n",
        "뉴런의 수학적 표현 - 두 벡터의 내적으로 쉽게 표현됨 =>수식\n",
        "전결합계층의 수학적 표현 - 여러 개의 뉴련을 한 곳에 모아둔 것, matrix 곱셈 연산으로 표현됨\n",
        "\n",
        "입력계층 -  아무런 연산 x, 신경망의 입력을 받아서 다음 계층으로 넘기는 역할, 무엇을 입력=>특징 추출 문제, 계층의 크기 = 노드 개수 = 입력 스칼라 수 = 입력 벡터의 길이\n",
        "\n",
        "은닉계층 - 입력계층과 연결된 fully connected layer, 얕은 신경망에서는 1개의 은닉 계층만 사용\n",
        "\n",
        "출력 계층 - 은닉 계층 다음에 오는 fully connected layer, 신경망 외부로 출력 신호 전달하는 데에 사용, 신경망의 기능은 출력 계층의 활성 함수에 의해 결정,\n",
        "\n",
        "4.회귀 문제의 이해\n",
        "\n",
        "선형 회귀 - 데이터를 가장 잘 표현하는 선형식을 찾는 동작\n",
        "\n",
        "단순 선형 회귀 - MSE를 최소로 하는 W를 찾아라\n",
        "\n",
        "다중 선형 회귀 - 벡터의 내적이 된다. 기하학적으로 변수가 하나 추가될 때마다 차원이 하나씩 추가됨(직선->평면->초평면)\n",
        "\n",
        "얕은 신경망과 회귀 알고리즘\n",
        "출력 계층이 선형 회귀와 동일, 출력 계층이 identity 함수, 입력 계층에서 은닉 계층으로 추가적인 변환이 있다는 것이 다른 점.\n",
        "\n",
        "5.이진 분류 문제\n",
        "\n",
        "로지스틱 회귀 : 범주형 데이터 (선형회귀는 0-1의 실수) 는 대상으로 하는 회귀, 분류 기법으로도 볼 수 있다.  => 선형회귀와 비슷하나, 범주형 데이터를 분류하는 방향으로 선을 긋는다.\n",
        "\n",
        "sigmoid function - 값이 작아질수록 0, 커질수록 1에 수렴. 확률, 모든 실수 입력 값에 대해 출력 정의됨. 0에 가까울수록 출력이 빠르게 변함, 모든 점에서 미분가능\n",
        "\n",
        "cross entropy error(교차 엔트로피 오차) - 정확히 맞추면 오차가 0, 틀릴수록 오차가 무한히 증가\n",
        "\n",
        "다중 로지스틱 회귀 - 기하학적으로 변수가 하나 추가될 때마다 차원이 하나씩 추가됨(직선->평면->초평면)\n",
        "\n",
        "얕은 신경망과 분류 알고리즘 - 얕은 신경망으로 classification을 수행할 때, 출력 계층은 로지스틱 회귀와 동일. 입력 계층에서 은닉 계층으로 추가적인 변환이 있는 것만 다름\n",
        "\n",
        "은닉 계층과 회귀 - 선형적으로 분리되는 은닉 계층을 통하여 로지스틱 회귀가 된다.  \n",
        "\n",
        "6.다중 분류 문제\n",
        "\n",
        "다중 클래스 분류 - 어떤 물체인지까지 표현해야함 (이진 분류는 예 아니오)\n",
        "원-핫-인코딩 - 한 개 값만 1, 나머지 값은 0인 벡터로 표현\n",
        "원핫인코딩의 희소 표현 - 벡터 전체 표현하지 말고 숫자 하나로 표현 가능\n",
        "\n",
        "얕은 신경망을 이용한 다중 클래스 분류 - 한가지 확률만 100%, 나머지는 0%, 실제 알고리즘 출력을 확률로 변환하기 위해 softmax함수를 사용한다.\n",
        "\n",
        "softmax function - 각 입력의 지수함수를 정규화한 것, 여러 경우의 수 중 한가지에 속할 확률\n",
        "\n",
        "softmax(다중) vs sigmiod(이진) -  sigmoid는 2가지 클래스를 구분하기 위해 1개의 입력을 받는다는 점이 특수한 경우\n",
        "\n",
        "cross entropy error(교차 엔트로피 오차) - 정답인 클래스에 대해서만 오차를 계산, 정확히 맞추면 오차가 0, 틀릴수록 오차가 무한히 증가\n",
        "오차를 내는 과정에서 정답 클래스만 비교하지만, 다중 클래스 분류의 활성함수인 softmax로 인해 다른 클래스에 대한 학습에도 영향을 준다."
      ],
      "metadata": {
        "id": "E4K3BQo-6QaR"
      }
    }
  ]
}