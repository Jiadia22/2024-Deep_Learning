05. 합성곱 신경망(cnn) 작동 원리

1. 합성곱 연산과 이미지 필터
아날로그 신호처리=> LTI시스템은 선형적이고 시간에 영향을 받지 않는 신호처리시스템
Dirac델타함수(디락델타함수) : 삼각형의 넓이는 1, h가 무한대로 가게 되면 높이h는 무한대로,2/h는 0으로 수렴. 삼각형을 쭉 늘려놓은 것

임펄스 응답 :  디락델타펑션이 한번 임펄스 했을 때 LTI함수의 응답이 나오게 되면 임펄스 응답 또는 필터라고 함. LTI시스템이 뭔지 모를 때 임펄스 넣어서 임펄스 응답 얻으면 뭔지 알 수 있음 

합성곱 연산 : 한 함수를 뒤집고 이동하면서 두 함수의 곱을 적분함
합성곱 연산과 LTI 시스템 : 잡음 있는 입력 *(컨볼루션) 잡음 제거 필터(임펄스응답값) = LTI시스템의 출력값 나옴 =>입력 신호에 임펄스 응답을 합성곱한 결과와 같다. 

이차원 신호와 흑백 이미지 : 흑백 영상은 각 픽셀이 0-1 사이의 실수로 된 2-D signal로 표현가능하다. 

이차원 신호와 컬러 이미지 : 컬러 영상은 RGB의 3채널로 구성된 2-D signal로 표현가능하다.

영상의 합성곱 계산 : 2-D 디지털 신호의 합성곱은 필터를 한 칸씩 옮기면서 영상과 겹치는 부분을 모두 곱해 합치면 된다. 

대표적 필터 2가지=>
잡음제거필터 : 2-D 가우시안 필터를 적용하면 흐려진 영상을 얻을 수 있다. 
영상이 흐려지는 대신 잡음을 제거하는 특성이 있다. 
미분 필터 : sobel filter를 적용하면 특정 방향으로 미분한 영상 얻을 수 있음. 해당 방향의 edge 성분을 추출하는 특성이 있다. 그림에서는 아래에서 위 방향으로의 그라디언트를 구한다.  세로 말고 가로가 잘 나온다. 

2.합성곱 계층
뉴런 : 가중치를 곱하는 형태로 sum계산
곱에서 합성곱으로 : 입력 뉴런 대신 입력 영상(채널)을, 가중치 대신 필터(임펄스 응답), 곱 대신 합성곱, 편향을 동일 = 이게 컨볼루션 레이어의 기본이 되는 뉴런이 된다. 

전결합계층을 합성곱 계층(convolutional layer)으로 : 
합성곱으로 이루어진 뉴런을 전결합 형태로 연결한 것을 합성곱 계층 
컨볼루셔널 필터(커널). 오른쪽처럼 피쳐맵 형태로 표현함

합성곱 계층의 의미 : 특징이 나타나는 위치에서 높은 값이 나옴.  이러한 위치를 찾아내는 것이 합성곱 계층의 임무

3. 기본적인 합성곱 신경망
기본구조 : 3채널짜리 영상 => 합성곱계층 - 풀링계층 - 활성함수

합성곱 계층 : 입력 영상 크기는 바뀌지 않고, 채널 개수가 바뀐다. 필터의 정보가 필요하다. 합성곱 계층에 의해서 추출된 결과는 공간적 특징이 있고 특징맵이라고 한다.
 
풀링계층 : 영상의 크기가 줄어들고, 정보가 종합된다. 
맥스풀링과 에버레지 풀링 등이 있다. 분류는 맥스풀링 잘 사용함

평탄화 :  모든 화소를 나열하여 하나의 벡터로 만드는 것을 평탄화.
합성곱 계층과 전결합 계층을 연결하는 역할을 한다. 

전결합계층 : 2개의 전결합 계층을 사용하여 최종 출력을 내어준다. 
앞에서 평탄화한 값을 입력을 해주고 얕은 신경망을 사용하는 것과 같다. 

왜 이런 구조를 쓰나? : 

리셉티브 필드 : 같은 크기의 필터여도, 풀링에 의해 작아진 특징 맵에 적용되면 원본 영상에서 차지하는 범위 넓어짐. 이 범위를 말한다. 

LeNet-5(르넷-5)
컨볼루션으로 6채널에서 16채널

VGG-16
컨볼루션과 렐루로 3채널->64채널 두 번 반복, 맥스풀링으로 크기 줄여주고 컨과렐로 채널수 늘려주는걸 2번 반복, 맥스풀링으로 줄이고 또 늘려주고 결국 7*7*512 나온다. 이걸 펴주고 풀리커넥티드로 반복해서 1000개의 클래스 구분으로 마무리 

4. 합성곱 신경망의 수식적 이해 
합성곱 계층 : 입력 피쳐맵이 여러차례 들어오면 여러 또 다른 채널 수만큼의 피처맵으로 출력해주는 그런 레이어이고, 곱하기 대신 이런 커널을 가지고 컨볼루션을 연산한다. 
 
오른쪽 사진은 중간 연산에 몇 개의 파라미터가 필요한지 표현

합성곱 계층의 필요성 : 영상을 입력으로 하는 것은 전결합 계층으로 할 것이 아니다. 

전결합계층의 수학적 표현 :  이래저래 매트릭스 형태로 표현이 된다. 

합성곱계층의 수학적 표현 :  입력과 출력 피쳐맵의 채널의 곱 크기만큼의 커널이 나타난다. 그것을 w라고 표현을 해둔다. 합성곱 계층은 입력채널*출력채널 개수 만큼의 합성곱 연산으로 이루어져있고 편향은 채널 하나마다 붙게끔 이루어진다. 

(텐서는 3차원 이상의 것들을 표현)

패딩의 필요성 : 합성곱 연산 시, 필터(커널)의 크기에 따라 영상의 크기가 줄어드는 문제가 있다. 

패딩 :  크기가 2n+1인 커널에 대해 상하좌우에 n개의 제로패딩을 해주면 크기가 줄어들지 않는다. 

스트라이드 : 커널을 이동시키는 거리를 stride, 이를 크게 하면 출력의 크기가 줄어든다. 연산량을 줄이기 위해서 종종 쓰인다. 

합성곱 계층의 특징맵=> 계층적으로 학습하는 모습을 볼 수 있다.

5. 배치 정규화 
일반 경사 하강법 : 그라디언트는 n개의 그라디언트의 평균을 구해서 넣어줌. 그라디언트를 한번 업데이트 하기 위해서 모든 학습 데이터를 사용한다. 
확률적 경사 하강법 : 데이터 수가 너무 크면 현실적이지 못하다. 그래서 데이터 선택의 과정이 들어가게 되고 배치사이즈 만큼의 그라디언트 평균을 구한다. 

미니 배치 학습법 : 학습데이터 전체를 한번 학습하는 것을 epoch, 한 번 그라디언트를 구하는 단위를 배치라고 한다. 

인터벌 코베리에이트 쉬프트 : 학습 과정에서 계층별로 입력의 데이터 분포가 달라지는 현상이다. 이것을 해결하기 위한 것이 배치정규화가 필요하다. 

배치정규화 : 각 배치별로 평균과 분산을 이용해 정규화하는 계층이 배치정규화 계층이다. 

학습 단계 : 각 배치별로 평균을 구하고 분산을 구하여 정규화를 한다. 
모든 계층의 피쳐가 동일한 스케일이 되어 학습률 결정에 유리하다. 
추가적인 스케일과 바이어스를 학습하여 엑티베이션에 적합한 분포로 변환.

추론단계 : 최근 n개에 대한 이동평균을 구한다. 추론 과정에서 평균과 분산을 이동 평균하여 고정. 스케일과 바이어스는 역전파로 학습된 값이다. 
따라서 트레이닝과 테스트는 동작이 다르다. 

6. 심화 합성곱 신경망1
합성곱 신경망 : 초창기 신경망인 VGG-16를 배워봤다. 컨볼루션과 렐루,맥스풀링 등이 사용되었었다. 

구글넷 : 구글넷이 더 에러율이 낫은 것을 볼 수 있음
구글넷의 구조 : 네트워크를 더 깊게 만들고자 하는 노력에서 나왔다.  

인셉션 모듈(naive version) : 다양한 크기의 합성곱 계층을 한번에 계산하는 모듈이다. 
다양한 크기의 피쳐들을 주체적으로 나눠서 학습을 한다는 특징이 있다. 

인셉션 모듈+dinension reduction : 연산량을 줄이기 위한 1*1 합성곱 계층, 이러한 구조를 바틀넥 구조라고 부른다. 

bottleneck 구조 : 리셉티브 필드를 유지하면서 파라미터 수와 연산량이 줄었다. 
32채널을 64채널로 변환하는 과정. 오른쪽 그림에서 16으로 줄여준 다음에 진행. 32에서 64채널로 증가하는 것을 동일하지만 파라미터 수가 절반정도로 줄었다. 

추가분류기 사용 : 여러 인셉션 모듈로 이루어져 있고, 역전파에서 기울기 소실이 발생하는 것을 방지하기 위해 추가 분류기를 달아서 이 문제를 방지하게 했다. 

7. 심화 합성곱 신경망2
residual network (ResNet) : 152레이어로 올려주고, 에러율이 낮아져 새로 등장했다. 
resnet구조 : 스킵커넥션이 주요한 역할을 한다. 

스킵커넥션 : 일반적인 구조는 컨볼루션 다음에 렐루를 반복, 
레지듀얼은 컨볼루션 레이어 렐루 컨볼루션 다음에 입력을 끌고와서 더해주고 다시 렐루로 출력. 피쳐를 추출하기 전 후를 더하는 특징이 있다. 
왼쪽과 오른쪽은 수학적으로 동치인 것을 확인할 수 있다. 

identity mapping : 레지듀얼과 다른점은 마지막에 렐루를 안한다. 그래서 이 구조에서는 identity mapping을 얻기 위해 pre-activation을 제안한다. 

pre-activation : 왼쪽이 레지듀얼 오른쪽이 아이덴티티 맵핑. 컨볼루션(weight)전에 먼저 렐루, 즉 프리 엑티베이션. 입력이 그대로 출력으로 이어져서 그라디언트 하이웨이가 형성된다. 순서만 달라졌는데 에러율이 뚝 떨어짐.

8. 심화 합성곱 신경망3
densely connected convnets (densenet) : 인풋이 있을 때 중간중간에 모든 레이어들이 다 연결이 되어있다. 

구조 : 컨볼루션레이어로 한번 피쳐 추출후 피쳐맵 만들었고, 댄스 블락으로 피쳐 뽑아줌, 컨볼루션으로 채널개수 조절, 풀링으로 영상크기 줄여줌 이렇게 반복 후 리니어를 거쳐 어떤 클래스인지 예측. 레즈넷과 같이 프리 엑티베이션 구조를 사용함. 

덴스블락 : 이전 특징 맵에서 누적해서 concatenate하는 결과와 같다. 

바틀넥 구조 : 안사용 시, 처음 입력된 채널 개수에 그로쓰레이트*개수
사용 시, 두개의 컨볼루션으로 쪼개져서 처음식을 작은숫자로 해준다.
연산 량이 급격히 증가하는 것을 막는다. 

덴스넷 구현 : 




























